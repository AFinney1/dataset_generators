{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/austin/anaconda3/envs/Py9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                   Date      Load\n",
       " 0  1/1/2022 12:00:00 AM  38145.79\n",
       " 1   1/1/2022 1:00:00 AM  37158.13\n",
       " 2   1/1/2022 2:00:00 AM  35966.24\n",
       " 3   1/1/2022 3:00:00 AM  35148.96\n",
       " 4   1/1/2022 4:00:00 AM  34610.33,\n",
       "                         Date      Load\n",
       " 8299   12/12/2022 7:00:00 PM  46775.92\n",
       " 8300   12/12/2022 8:00:00 PM  46217.27\n",
       " 8301   12/12/2022 9:00:00 PM  44998.13\n",
       " 8302  12/12/2022 10:00:00 PM  42774.80\n",
       " 8303  12/12/2022 11:00:00 PM  40435.58)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ercot_2022_actual_load_df = pd.read_csv('downloaded/20220101-20221212 ERCOT Actual Load.csv')\n",
    "ercot_2022_actual_load_df.head(), ercot_2022_actual_load_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3169, 2), (5135, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def train_test_split(df, test_size=0.2):\n",
    "    train_size = int(len(df) * (1 - test_size))\n",
    "    train_set = df[:train_size]\n",
    "    test_set = df[train_size:]\n",
    "    return train_set, test_set\n",
    "\n",
    "def train_test_split_by_date(df):\n",
    "    train_set = df[df['Date'] < '2022-10-01']\n",
    "    test_set = df[df['Date'] >= '2022-10-01']\n",
    "    return train_set, test_set\n",
    "    \n",
    "\n",
    "train_set, test_set = train_test_split_by_date(ercot_2022_actual_load_df)\n",
    "train_set.shape, test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(torch.utils.data.Dataset):   \n",
    "    def __init__(self, X, y, seq_len=1):\n",
    "        self.X = X\n",
    "        self.y = X\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(index)\n",
    "        a, b = self.X[index:index+self.seq_len], self.y[index+self.seq_len]\n",
    "        #print(a.shape, b.shape)\n",
    "        return(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3169]), torch.Size([5135]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def df_to_tensor(df):\n",
    "    X = torch.tensor(df['Load'].values).float()\n",
    "    return X\n",
    "#print(test_set.values[1663])\n",
    "x_time_series = df_to_tensor(train_set)\n",
    "y_time_series = df_to_tensor(test_set)\n",
    "x_time_series.shape, y_time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 73\n",
    "batch_size = seq_len\n",
    "train_dataset = TimeseriesDataset(x_time_series, y_time_series, seq_len=seq_len)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "\n",
    "# for nth_batch, (batch, _) in enumerate(train_dataset):\n",
    "#     print(f'Batch {nth_batch}:')\n",
    "#     print(f'batch shape: {batch.shape}')\n",
    "#     print(f'batch: {batch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "dims = seq_len\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, input_dim, output_dim, hidden_dim, num_layers, num_heads):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = nn.TransformerEncoder(\n",
    "      nn.TransformerEncoderLayer(input_dim, num_heads, hidden_dim),\n",
    "      num_layers\n",
    "    )\n",
    "\n",
    "    self.decoder = nn.TransformerDecoder(\n",
    "      nn.TransformerDecoderLayer(output_dim, num_heads, hidden_dim),\n",
    "      num_layers\n",
    "    )\n",
    "\n",
    "    self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, src, trg):\n",
    "    context = self.encoder(src)\n",
    "    output = self.decoder(trg, context)\n",
    "    return self.output_layer(output)\n",
    "\n",
    "Transformer = Transformer(input_dim=dims, output_dim=dims, hidden_dim=seq_len, num_layers=4, num_heads=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 42/44 [00:01<00:00, 33.74it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3169 is out of bounds for dimension 0 with size 3169",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m   Transformer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mtransformer_model.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39melse\u001b[39;00m:    \n\u001b[0;32m---> 25\u001b[0m     train_transformer(Transformer, train_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m     26\u001b[0m     torch\u001b[39m.\u001b[39msave(Transformer\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mtransformer_model.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [26], line 8\u001b[0m, in \u001b[0;36mtrain_transformer\u001b[0;34m(model, train_loader, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m----> 8\u001b[0m   \u001b[39mfor\u001b[39;00m batch, _ \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m      9\u001b[0m     \u001b[39m#print(batch.shape)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m#batch2 = batch.transpose(-2, -1)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[39m#print(batch.shape)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     output \u001b[39m=\u001b[39m model(batch, batch)\n\u001b[1;32m     14\u001b[0m     loss \u001b[39m=\u001b[39m criterion(output, batch)\n",
      "File \u001b[0;32m~/anaconda3/envs/Py9/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Py9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/Py9/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/Py9/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/Py9/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [22], line 12\u001b[0m, in \u001b[0;36mTimeseriesDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m     11\u001b[0m     \u001b[39m#print(index)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     a, b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX[index:index\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my[index\u001b[39m+\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len]\n\u001b[1;32m     13\u001b[0m     \u001b[39m#print(a.shape, b.shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m(a, b)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3169 is out of bounds for dimension 0 with size 3169"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "def train_transformer(model, train_loader, epochs):\n",
    "  criterion = nn.MSELoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    for batch, _ in tqdm(train_loader):\n",
    "      #print(batch.shape)\n",
    "      #batch2 = batch.transpose(-2, -1)\n",
    "\n",
    "      #print(batch.shape)\n",
    "      output = model(batch, batch)\n",
    "      loss = criterion(output, batch)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "if os.path.exists('transformer_model.pt'):\n",
    "  Transformer = torch.load('transformer_model.pt')\n",
    "  \n",
    "else:    \n",
    "    train_transformer(Transformer, train_loader, epochs=100)\n",
    "    torch.save(Transformer.state_dict(), 'transformer_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6643</th>\n",
       "      <td>10/4/2022 8:00:00 PM</td>\n",
       "      <td>50943.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6644</th>\n",
       "      <td>10/4/2022 9:00:00 PM</td>\n",
       "      <td>48196.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6645</th>\n",
       "      <td>10/4/2022 10:00:00 PM</td>\n",
       "      <td>44963.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6646</th>\n",
       "      <td>10/4/2022 11:00:00 PM</td>\n",
       "      <td>41779.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6647</th>\n",
       "      <td>10/5/2022 12:00:00 AM</td>\n",
       "      <td>38623.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date      Load\n",
       "6643   10/4/2022 8:00:00 PM  50943.27\n",
       "6644   10/4/2022 9:00:00 PM  48196.03\n",
       "6645  10/4/2022 10:00:00 PM  44963.52\n",
       "6646  10/4/2022 11:00:00 PM  41779.37\n",
       "6647  10/5/2022 12:00:00 AM  38623.86"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'trg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m       \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOutput: \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m test_transformer(Transformer, train_loader)\n",
      "Cell \u001b[0;32mIn [15], line 5\u001b[0m, in \u001b[0;36mtest_transformer\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m   \u001b[39mfor\u001b[39;00m batch, _ \u001b[39min\u001b[39;00m test_loader:\n\u001b[1;32m      4\u001b[0m     \u001b[39m#batch = batch.permute(1, 0, 2)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     output \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOutput: \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Py9/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'trg'"
     ]
    }
   ],
   "source": [
    "def test_transformer(model, test_loader):\n",
    "  with torch.no_grad():\n",
    "    for batch, _ in test_loader:\n",
    "      #batch = batch.permute(1, 0, 2)\n",
    "      output = model(batch)\n",
    "      print(f'Input: {batch}')\n",
    "      print(f'Output: {output}')\n",
    "test_transformer(Transformer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m             fig\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m plot_predictions(Transformer, train_loader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Transformer' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.express as px \n",
    "def plot_predictions(model, test_loader):\n",
    "    \"\"\"Using Plotly, plot predictions of the model on the test set against the actual values for a single batch. The y-axis is 'Load'. The x-axis is the index of the test set.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for batch, _ in test_loader:\n",
    "            output = model(batch, batch)\n",
    "            output = output.flatten()\n",
    "            batch = batch.flatten()\n",
    "            df = pd.DataFrame({'Load': output, 'Predicted Load': batch})\n",
    "            df = df.reset_index()\n",
    "            df = df.melt(id_vars='index', value_vars=['Load', 'Predicted Load'])\n",
    "            fig = px.line(df, x='index', y='value', color='variable')\n",
    "            fig.show()\n",
    "            break\n",
    "plot_predictions(Transformer, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (norm1): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (norm1): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (norm1): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (norm1): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (norm1): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (norm1): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (norm1): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=73, out_features=73, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=73, out_features=73, bias=True)\n",
      "        (norm1): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((73,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=73, out_features=73, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def summarize_transformer(model):\n",
    "  print(model)\n",
    "summarize_transformer(Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('Py9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44fe909950c17bf228e77f937df459853b2996d211099b019aac185bdac4e51f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
